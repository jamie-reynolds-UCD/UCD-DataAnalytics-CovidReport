{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-absorption",
   "metadata": {},
   "source": [
    "# 1. Prepare a data quality report for your CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('covid19-cdc-19200690.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-constraint",
   "metadata": {},
   "source": [
    "### Check how many rows and columns your CSV has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-brake",
   "metadata": {},
   "source": [
    "The CSV has 10000 rows and 12 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-honolulu",
   "metadata": {},
   "source": [
    "### Print the first and the last 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first five rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last five rows\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-performance",
   "metadata": {},
   "source": [
    "## Convert the features to their appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-girlfriend",
   "metadata": {},
   "source": [
    "Based on examining the data in a spreadsheet program, 4 of the features are continuous and the rest categorical.\n",
    "\n",
    "- 'cdc_case_earliest_dt', 'cdc_report_dt', 'pos_spec_dt' and 'onset_dt' will all be converted to type datetime64[ns], as their values are all dates.\n",
    "- The rest of the columns will be converted to type 'category' as they all have 10 or fewer unique values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-steam",
   "metadata": {},
   "source": [
    "## Continuous columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-performance",
   "metadata": {},
   "source": [
    "- **First rename the columns to be slightly shorter and easier to understand**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'cdc_case_earliest_dt':'earliest_date', 'cdc_report_dt':'report_date',\n",
    "                   'pos_spec_dt':'posSpec_date', 'onset_dt':'onset'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-jason",
   "metadata": {},
   "source": [
    "- **select the columns for the continuous table, and assign the list of these column names to the variable 'continuous'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = df[['earliest_date','report_date','posSpec_date','onset']].columns\n",
    "continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-composition",
   "metadata": {},
   "source": [
    "- **convert each of these features to datetime object, making sure to specify that the format of the entries is in the order: day, month, year. This will make sure that the values are stored properly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in df[continuous]:\n",
    "    df[date] = pd.to_datetime(df[date], format='%d/%m/%Y')\n",
    "\n",
    "df[continuous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[continuous].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-exhibition",
   "metadata": {},
   "source": [
    "We can see that the continuous columns are all of type datetime64[ns] now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-technology",
   "metadata": {},
   "source": [
    "## Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-genre",
   "metadata": {},
   "source": [
    "- **First – as I did with the continuous columns – rename the columns to be slightly shorter and easier to understand**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'current_status':'status', 'race_ethnicity_combined':'race_ethnicity','hosp_yn':'hosp',\n",
    "                  'icu_yn':'icu','death_yn':'death','medcond_yn':'medcond'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-handling",
   "metadata": {},
   "source": [
    "- **select the columns for the categorical table, and assign the list of these column names to the variable 'categorical'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = df[['status','sex','age_group','race_ethnicity','hosp','icu','death','medcond']].columns\n",
    "categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-witch",
   "metadata": {},
   "source": [
    "- **convert each of these features to type 'category'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-pacific",
   "metadata": {},
   "source": [
    "We can see that the categorical columns are all of type category now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-wiring",
   "metadata": {},
   "source": [
    "## Look for duplicate rows and columns. Consider whether it makes sense to keep them or drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate rows\n",
    "print(\"Number of duplicated (excluding first) rows in the CSV file is:\", df.duplicated().sum())\n",
    "print(\"Number of duplicated (including first) rows in the CSV file is:\", df[df.duplicated(keep=False)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate columns\n",
    "dfT = df.T\n",
    "print(\"Number of duplicated (excluding first) columns in the CSV file is:\", dfT.duplicated().sum())\n",
    "print(\"Number of duplicated (including first) columns in the CSV file is:\", dfT[dfT.duplicated(keep=False)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-nickel",
   "metadata": {},
   "source": [
    "- **There are 473 rows that are duplicates.**\n",
    "- **There are no duplicate columns.**\n",
    "- **There are no constant columns – all columns have at least 2 unique values, which can be seen in the results displayed by running the nunique() function on the dataframe**\n",
    "\n",
    "I think it makes sense to keep the duplicate rows. There are no details that can uniquely identify a patient, such as an email or patient ID number and it is possible that the duplicate rows are entries of different people with the same details in terms of this dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-promise",
   "metadata": {},
   "source": [
    "### No duplicate columns were found. But I will check that duplicated() function works on a small dataframe that contains dupicate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'col1': [1, 1, 5], 'col2': [1, 1, 4], 'col3': [1, 1, 5], 'col4': [1, 1, 5], 'col2': [1, 1, 4]}\n",
    "sampleDuplicate = pd.DataFrame(data=data)\n",
    "sampleDuplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-lyric",
   "metadata": {},
   "source": [
    "This dataframe should have 2 columns (excluding first) that are duplicates, and 1 row (excluding first) that are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows\n",
    "print('Number of duplicate rows (excluding first) is:', sampleDuplicate.duplicated().sum())\n",
    "print(\"Number of duplicated rows (including first) in the CSV file is:\", sampleDuplicate[sampleDuplicate.duplicated(keep=False)].shape[0])\n",
    "\n",
    "# columns\n",
    "print('Number of duplicate columns (excluding first) is:', sampleDuplicate.T.duplicated().sum())\n",
    "print(\"Number of duplicated columns (including first) in the CSV file is:\", sampleDuplicate.T[sampleDuplicate.T.duplicated(keep=False)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-sailing",
   "metadata": {},
   "source": [
    "The duplicated() function appears to be working correctly, therefore it can be assumed that there are no duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-shaft",
   "metadata": {},
   "source": [
    "## Descriptive statistics: Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[continuous].describe(datetime_is_numeric=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-address",
   "metadata": {},
   "source": [
    "We know the data type for each feature and we know the cardinality for each feature.\n",
    "Now we can find out:\n",
    "- the range of values each feature has\n",
    "- the number of instances of the 15 most common values for each feature\n",
    "- the number of instances of the 15 least common values for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each continuous feature, we display the range of values it takes.\n",
    "# We also display the number of instances each of its values has.\n",
    "\n",
    "for feature in continuous:\n",
    "    print(feature)\n",
    "    print(\"----------\\n\")\n",
    "    print(\"Range of {} is:\".format(feature), (df[feature].max() - df[feature].min()))\n",
    "    print(\"Latest date is {}:\".format(df[feature].max()))\n",
    "    print(\"Earliest date is {}:\".format(df[feature].min()))\n",
    "    print(\"----------\")\n",
    "    print(\"15 most common values:\")\n",
    "    print('{0:.5}  {1}'.format(\"Value\", \"Number of Instances\"))\n",
    "    print(df[feature].value_counts().nlargest(15), \"\\n\")\n",
    "    print(\"15 least common values:\")\n",
    "    print('{0:.5}  {1}'.format(\"Value\", \"Number of Instances\"))\n",
    "    print(df[feature].value_counts().nsmallest(15), \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-coffee",
   "metadata": {},
   "source": [
    "### Do any continuous features aside from 'earliest_date' appear on their own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many have just report_date?\n",
    "\n",
    "reportCount = []\n",
    "for i in range(len(df)):\n",
    "    if not pd.isnull(df.loc[i]['report_date']):\n",
    "        if pd.isnull(df.loc[i]['posSpec_date']) and pd.isnull(df.loc[i]['onset']):\n",
    "            reportCount.append(i)\n",
    "            \n",
    "print(f'There are {len(reportCount)} rows whose continuous features only contain values for \\\n",
    "\"report_count\" as well as \"earliest_date\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[reportCount].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many just have onset?\n",
    "\n",
    "onsetCount = []\n",
    "for i in range(len(df)):\n",
    "    if not pd.isnull(df.loc[i]['onset']):\n",
    "        if pd.isnull(df.loc[i]['posSpec_date']) and pd.isnull(df.loc[i]['report_date']):\n",
    "            onset.append(i)\n",
    "            \n",
    "print(f'There are {len(onsetCount)} rows whose continuous features only contain values for \\\n",
    "\"report_count\" as well as \"earliest_date\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-accent",
   "metadata": {},
   "source": [
    "### Running Assessment:\n",
    "\n",
    "- Examining the gaps / relationships between the minimum and maximum values and the quartile ranges, the features listed below may contain significant outliers. This is not a definitive list. More insight will be gained from visual analysis of these data presented on histograms below.\n",
    "<br>\n",
    "\n",
    " -  earliest_date\n",
    " -  report_date\n",
    " -  posSpec_date\n",
    " -  onset\n",
    "<br><br>\n",
    "\n",
    "-  It is clear from both visual analysis of the rows and from this initial descriptive analysis that there are no continuous outlier values. For each of the continuous features, the most common of the values all occur after the 75th percentile, and the least common values all occur before or up to the 25th percentile. This is consistent with the fact that the earlier dates are earlier in the pandemic, and there would be fewer COVID-19 cases during the earlier period, as the disease had not had time to spread yet. There does not appear to be any special value that skews the data.\n",
    "<br><br>\n",
    "- Evidence of the above observation can be seen in the test on the earliest_date column below:\n",
    "  - the largest range of dates occurs between minimum value and the 25th percentile.\n",
    "    - a larger range of dates between means a lower rate of COVID-19 cases over this period.\n",
    "  - the smallest range of dates occurs between the 75th percentile and the maximum value.\n",
    "    - a smaller range of dates means a higher rate of COVID-19 cases over this period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-positive",
   "metadata": {},
   "source": [
    "### earliest_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_min = pd._libs.tslibs.timestamps.Timestamp('2020-01-05')\n",
    "early_twentyFive = pd._libs.tslibs.timestamps.Timestamp('2020-07-26')\n",
    "early_fifty = pd._libs.tslibs.timestamps.Timestamp('2020-11-06')\n",
    "early_seventyFive = pd._libs.tslibs.timestamps.Timestamp('2020-12-15')\n",
    "early_max = pd._libs.tslibs.timestamps.Timestamp('2021-01-16')\n",
    "\n",
    "print(f'There are {early_twentyFive - early_min} between the earliest date and the that of the 25th percentile.')\n",
    "print(f'There are {early_fifty - early_twentyFive} between the 25th percentile and the that of the 50th percentile.')\n",
    "print(f'There are {early_seventyFive - early_fifty} between the 50th percentile and the that of the 75th percentile.')\n",
    "print(f'There are {early_max - early_seventyFive} between the 75th percentile and the that of the latest date.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-prairie",
   "metadata": {},
   "source": [
    "### report_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_min = pd._libs.tslibs.timestamps.Timestamp('2020-01-05')\n",
    "report_twentyFive = pd._libs.tslibs.timestamps.Timestamp('2020-08-15')\n",
    "report_fifty = pd._libs.tslibs.timestamps.Timestamp('2020-11-11')\n",
    "report_seventyFive = pd._libs.tslibs.timestamps.Timestamp('2020-12-21')\n",
    "report_max = pd._libs.tslibs.timestamps.Timestamp('2021-01-29')\n",
    "\n",
    "print(f'There are {report_twentyFive - report_min} between the earliest date and the that of the 25th percentile.')\n",
    "print(f'There are {report_fifty - report_twentyFive} between the 25th percentile and the that of the 50th percentile.')\n",
    "print(f'There are {report_seventyFive - report_fifty} between the 50th percentile and the that of the 75th percentile.')\n",
    "print(f'There are {report_max - report_seventyFive} between the 75th percentile and the that of the latest date.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-screen",
   "metadata": {},
   "source": [
    "### posSpec_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_min = pd._libs.tslibs.timestamps.Timestamp('2020-03-06')\n",
    "pos_twentyFive = pd._libs.tslibs.timestamps.Timestamp('2020-07-02')\n",
    "pos_fifty = pd._libs.tslibs.timestamps.Timestamp('2020-10-11')\n",
    "pos_seventyFive = pd._libs.tslibs.timestamps.Timestamp('2020-12-02')\n",
    "pos_max = pd._libs.tslibs.timestamps.Timestamp('2021-01-25')\n",
    "\n",
    "print(f'There are {pos_twentyFive - pos_min} between the earliest date and the that of the 25th percentile.')\n",
    "print(f'There are {pos_fifty - pos_twentyFive} between the 25th percentile and the that of the 50th percentile.')\n",
    "print(f'There are {pos_seventyFive - pos_fifty} between the 50th percentile and the that of the 75th percentile.')\n",
    "print(f'There are {pos_max - pos_seventyFive} between the 75th percentile and the that of the latest date.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-flooring",
   "metadata": {},
   "source": [
    "### Onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_min = pd._libs.tslibs.timestamps.Timestamp('2020-01-05')\n",
    "onset_twentyFive = pd._libs.tslibs.timestamps.Timestamp('2020-07-14')\n",
    "onset_fifty = pd._libs.tslibs.timestamps.Timestamp('2020-10-18')\n",
    "onset_seventyFive = pd._libs.tslibs.timestamps.Timestamp('2020-12-02')\n",
    "onset_max = pd._libs.tslibs.timestamps.Timestamp('2021-01-27')\n",
    "\n",
    "print(f'There are {onset_twentyFive - onset_min} between the earliest date and the that of the 25th percentile.')\n",
    "print(f'There are {onset_fifty - onset_twentyFive} between the 25th percentile and the that of the 50th percentile.')\n",
    "print(f'There are {onset_seventyFive - onset_fifty} between the 50th percentile and the that of the 75th percentile.')\n",
    "print(f'There are {onset_max - onset_seventyFive} between the 75th percentile and the that of the latest date.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-nursing",
   "metadata": {},
   "source": [
    "## Check logical integrity of data\n",
    "The following are some checks to see that the data makes sense. Rows that fail these tests may be dropped from the dataframe, or have values replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-leisure",
   "metadata": {},
   "source": [
    "### Test 1:\n",
    "- are there any rows where the 'earliest_date' column is a later date than any of the other columns?\n",
    "  - this should be impossible as the 'earliest_date' column should be earlier than or equal to all other date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_rows_earliest_date = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i]['report_date'] < df.loc[i]['earliest_date'] or df.loc[i]['posSpec_date'] < df.loc[i]['earliest_date']\\\n",
    "    or df.loc[i]['onset'] < df.loc[i]['earliest_date']:\n",
    "        failed_rows_earliest_date.append(i)\n",
    "print(f'There are {len(failed_rows_earliest_date)} rows that have a date earlier than the \"earliest_column\" date.')\n",
    "print()\n",
    "print('Rows that have a date earlier than the \"earliest_date\" column:', failed_rows_earliest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[failed_rows_earliest_date].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-israel",
   "metadata": {},
   "source": [
    "### Test 2:\n",
    "- are there any rows that have a 'probable_case' for the status column, and a not null value for the posSpec_date column?\n",
    "  - this should be impossible, because if the case is not marked as 'Laboratory-confirmed case', there should be no date for when a positive specimen sample was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_rows_PosSpec_date_status = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if not pd.isnull(df.loc[i]['posSpec_date']) and df.loc[i]['status'] == 'Probable Case':\n",
    "        failed_rows_PosSpec_date_status.append(i)\n",
    "        \n",
    "print(f'There are {len(failed_rows_PosSpec_date_status)} rows that have a \"probable_case\" for the status column, \\\n",
    "and a not null value for the posSpec_date column.')\n",
    "print()\n",
    "print(f'Rows that have a \"probable_case\" for the status column,\\\n",
    "and a not null value for the posSpec_date column: {failed_rows_PosSpec_date_status}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[failed_rows_PosSpec_date_status].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-private",
   "metadata": {},
   "source": [
    "### Test 3:\n",
    "- are there any rows that have a 'yes' value for the icu column, and 'no', 'unknown', or 'missing' for the hosp column?\n",
    "  - this should be impossible if the hosp value is 'no'.\n",
    "  - it should also indicate the value for the hosp column if the hosp value is 'unknown' or 'missing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_rows_hosp_icu = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i]['icu'] == 'Yes' and (df.loc[i]['hosp'] == 'No' or df.loc[i]['hosp'] == 'Unknown'\\\n",
    "                                      or df.loc[i]['hosp'] == 'Missing'):\n",
    "        failed_rows_hosp_icu.append(i)\n",
    "\n",
    "print(f'There are {len(failed_rows_hosp_icu)} rows that have a \"yes\" value for the icu column \\\n",
    "and \"no\", \"missing\", or \"unknown\" for the hosp column.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-brooks",
   "metadata": {},
   "source": [
    "## Save your updated/cleaned data frame to a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('covid19-cdc-19200690_cleaned_1_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-weather",
   "metadata": {},
   "source": [
    "## Prepare a table with descriptive statistics for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[continuous].describe(datetime_is_numeric=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-booking",
   "metadata": {},
   "source": [
    "- **Add column for % missing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns_missing  = 100 * (df[continuous].isnull().sum()/df.shape[0])\n",
    "df_continuous_missing = pd.DataFrame(continuous_columns_missing, columns=['% missing'])\n",
    "df_continuous_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-sending",
   "metadata": {},
   "source": [
    "- **Add column for cardinality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_card = df[continuous].nunique()\n",
    "df_continuous_card = pd.DataFrame(continuous_card, columns=['cardinality'])\n",
    "df_continuous_card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-warren",
   "metadata": {},
   "source": [
    "- **Create the final continuous data table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_continuous = df[continuous].describe(datetime_is_numeric=True).T\n",
    "\n",
    "# Put the columns together to prepare the final table for numeric_columns\n",
    "df_continuous_columns_data_quality_report_table = pd.concat([df_table_continuous, df_continuous_missing, df_continuous_card], axis=1)\n",
    "\n",
    "# save to a csv file\n",
    "df_continuous_columns_data_quality_report_table.to_csv('df_continuous_columns_data_quality_report_table.csv')\n",
    "\n",
    "df_continuous_columns_data_quality_report_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-concord",
   "metadata": {},
   "source": [
    "## Prepare a table with descriptive statistics for all the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical = df[categorical].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-vacation",
   "metadata": {},
   "source": [
    "- **Look at the values taken by each categorical feature as a propportion, including the NaN values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df[categorical]:\n",
    "    print(\"\\n\" + column)\n",
    "    print(df[column].value_counts(dropna=False, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-relation",
   "metadata": {},
   "source": [
    "- **store the 2ndmode and 2ndmode % values in a new dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical_mode = pd.DataFrame(index = df_categorical.index, columns=['mode', 'freq_mode','%mode', '2ndmode', 'freq_2ndmode','%2ndmode'])\n",
    "df_categorical_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the values taken by each categorical feature\n",
    "for column in categorical:\n",
    "    print(\"\\n\" + column)\n",
    "    print(df[column].value_counts())\n",
    "    print(df[column].value_counts().keys())\n",
    "    \n",
    "    df_categorical_mode.loc[column]['mode'] = df[column].value_counts().keys()[0]\n",
    "    df_categorical_mode.loc[column]['freq_mode'] = df[column].value_counts()[df_categorical_mode.loc[column]['mode']]\n",
    "    df_categorical_mode.loc[column]['%mode'] = 100 * (df[column].value_counts(normalize=True)[df_categorical_mode.loc[column]['mode']])\n",
    "    \n",
    "    if df[column].value_counts().size > 1:\n",
    "        df_categorical_mode.loc[column]['2ndmode'] = df[column].value_counts().keys()[1]\n",
    "        df_categorical_mode.loc[column]['freq_2ndmode'] = df[column].value_counts()[df_categorical_mode.loc[column]['2ndmode']]\n",
    "        df_categorical_mode.loc[column]['%2ndmode'] = 100 * (df[column].value_counts(normalize=True)[df_categorical_mode.loc[column]['2ndmode']])\n",
    "    else: df_categorical_mode.loc[column] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-recognition",
   "metadata": {},
   "source": [
    "- add a column for % missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_null  = 100 * (df[categorical].isnull().sum()/df.shape[0])\n",
    "\n",
    "df_categorical_columns_null = pd.DataFrame(categorical_columns_null, columns=['% null'])\n",
    "\n",
    "df_categorical_columns_null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-induction",
   "metadata": {},
   "source": [
    "As we can see, there are no null values in the categorical features. But from reading the values present in each of the categorical columns, there are entries of 'Missing' and 'Unknown'. These values will not contribute to the understanding of the problem. If there are high percentages of these values in a column, it may be suitable to drop the column. I will find out:\n",
    "- what percentage of 'Missing' or 'Unknown' values are in each column\n",
    "- in which rows, there are values of 'Missing' or 'Unknown'. Some rows may need to be dropped due to a large amount of missing or unknown values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical_columns_missing = pd.DataFrame(columns=['% Missing/Unknown'])\n",
    "\n",
    "for i in range (len(categorical)):\n",
    "    df_categorical_columns_missing.loc[df[categorical].columns[i]] =\\\n",
    "    (100 * (df[df[categorical].columns[i]].str.contains('Unknown').sum()/df.shape[0]))\\\n",
    "    + (100 * (df[df[categorical].columns[i]].str.contains('Missing').sum()/df.shape[0]))\n",
    "    \n",
    "df_categorical_columns_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-colors",
   "metadata": {},
   "source": [
    "- **Create the final categorical data table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_categorical = df[categorical].describe().T\n",
    "\n",
    "# Put the columns together to prepare the final table for numeric_columns\n",
    "df_categorical_columns_data_quality_report_table = pd.concat([df_table_categorical, df_categorical_mode, df_categorical_columns_missing], axis=1)\n",
    "\n",
    "# save to a csv file\n",
    "df_categorical_columns_data_quality_report_table.to_csv('df_categorical_columns_data_quality_report_table.csv')\n",
    "\n",
    "df_categorical_columns_data_quality_report_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-logging",
   "metadata": {},
   "source": [
    "## Plot histograms for all the continuous features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-hearing",
   "metadata": {},
   "source": [
    "### - Plot histograms summary sheet for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.style.use('classic')\n",
    "df.hist(figsize=(20, 20), xrot = 90, edgecolor='white', alpha=0.5)\n",
    "\n",
    "plt.savefig('covid19-cdc-19200690-DataQualityReport-ContinuousFeatures-histograms-1-1.pdf')\n",
    "\n",
    "# print(plt.style.available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-probability",
   "metadata": {},
   "source": [
    "### - Plot histograms individual sheet for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in continuous:\n",
    "    plt.figure()\n",
    "    plt.style.use('classic')\n",
    "    plt.title(col)\n",
    "    plt.ylabel('number of cases')\n",
    "    df[col].hist(figsize=(10, 5), xrot = 90, edgecolor='white', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-diamond",
   "metadata": {},
   "source": [
    "## Plot box plots for all the continuous features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-trash",
   "metadata": {},
   "source": [
    "It was decided to not use box plots, as there are no outliers for dates. The data is measured from one date to another, and there are no data outside these dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-reception",
   "metadata": {},
   "source": [
    "## Plot bar plots for all the categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-basic",
   "metadata": {},
   "source": [
    "### - Plot bar chart summary sheet for all the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar charts for all categorical features and save them in a single PDF file\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.tight_layout()\n",
    "\n",
    "# We can set the parameters for .value_counts() to not drop the na. This allows us to see missing data in our \n",
    "# categorical features.\n",
    "with PdfPages('covid19-cdc-19200690-DataQualityReport-CategoricalFeatures_barcharts_1-1.pdf') as pp:\n",
    "    for column in categorical:\n",
    "        f = df[column].value_counts(dropna=False).plot(kind='bar', title=column, figsize=(10,12), alpha=0.5)\n",
    "        plt.ylabel('number of entries')\n",
    "        if column == 'status':\n",
    "            plt.xticks(rotation=0)\n",
    "        elif column == 'age_group':\n",
    "            plt.xticks(rotation=30)\n",
    "        elif column == 'race_ethnicity':\n",
    "            plt.xticks(rotation=12)\n",
    "        pp.savefig(f.get_figure())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_continuous_columns_data_quality_report_table:\n",
    "    print(df_continuous_columns_data_quality_report_table[row])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-example",
   "metadata": {},
   "source": [
    "# 2). Prepare a data quality plan for the cleaned CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-reproduction",
   "metadata": {},
   "source": [
    "## List of issues identified in the data quality Report:\n",
    "- Duplicate rows: there are 473 rows that are duplicates.\n",
    "- Logical integrity – 'earliest_date': 68 rows have a column that has an earlier date than the 'earliest_date' column.\n",
    "- Logical integrity – 'status': 215 rows have a value of 'Probable Case' for the 'status' column, but also have a value for the 'posSpec_date' column – which indicates that they are a confirmed case.\n",
    "- Continuous features – 'posSpec_date': the 'posSpec_date' column is missing 72.74% of its values.\n",
    "- Continuous features – 'onset': the 'onset' column is missing 49.16% of its values.\n",
    "- Categorical features – 'icu': the 'icu' column is missing 89.35% of its values.\n",
    "- Categorical features – 'medcond': the 'medcond' column is missing 82.83% of its values.\n",
    "- Categorical features – 'sex': the 'sex' column has 0.82 % of its values missing.\n",
    "- Categorical features – 'age_group': the 'age_group' column has 0.1% of its values missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-aspect",
   "metadata": {},
   "source": [
    "## Propose solutions to deal with the problems identified.  \n",
    "#### 1. Duplicate rows\n",
    "- No action will be taken on duplicate rows: there is nothing to uniquely a person in the dataset. Therefore there is no indicator that these rows are duplicates rather than entries for people who have the same details listed in the dataset. This is why I chose to keep them rather than drop them.  \n",
    "\n",
    "#### 2. Logical Integrity – 'earliest_date'\n",
    "- The rows that fail this test will have the value of their 'earliest_date' column changed to whatever the minimum value for the continuous features of that row is. The 'earliest_date' column is simply the column that has the earliest date available for the entry, so there is no need to drop the rows here.\n",
    "\n",
    "#### 3. Logical Integrity – 'status'\n",
    "- The rows that fail this test will be dropped from the dataset. There is no way of knowing for these rows, which entry was the mistake: the entry in the ‘status’ column or the entry in the ‘posSpec_date’ column.\n",
    "\n",
    "#### 4. Continuous features – 'posSpec_date'\n",
    "- The 'posSpec_date' column will be dropped from the dataset. It is missing 72.74% of its values, and there are no rows reliant on this column alone for dates.\n",
    "\n",
    "#### 5. Continuous features – 'onset'\n",
    "- The 'onset' column will be dropped from the dataset. It is missing 49.16% of its values. This, in combination with the imperfect nature of this data – it is easily misremembered or mistaken by pepople – has led to the decision to the drop this column.\n",
    "\n",
    "#### 6. Categorical features – 'icu'\n",
    "- The 'icu' column will be dropped from the dataset as it is missing 89.35% of its values.\n",
    "\n",
    "#### 7. Categorical features – 'medcond'\n",
    "- The 'medcond' column will be dropped from the dataset as it is missing 82.83% of its values.\n",
    "\n",
    "#### 8. Categorical features – 'sex'\n",
    "- Any rows with a missing value for the 'sex' column will be dropped from the dataset. It will not result in much lost data – just 0.82% – and knowledge of the sex of the patient is important for identifying trends related to the target feature.\n",
    "\n",
    "#### 9. Categorical features – 'age_group'\n",
    "- Any rows with a missing value for the 'age_group' column will be dropped from the dataset. This will not lose much data – just 0.1% – and knowing the age group of each patient is useful information in identifying trends related to the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-deputy",
   "metadata": {},
   "source": [
    "## Apply your solutions to obtain a new CSV file where the identified data quality issues were addressed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-motion",
   "metadata": {},
   "source": [
    "#### 1. Duplicate rows\n",
    "- nothing to be done here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-approval",
   "metadata": {},
   "source": [
    "#### 2. Logical Integrity – 'earliest_date'\n",
    "- Here we iterating through the list of row indexes where the row contains a continuous feature whose date is earlier than that of its 'earliest_date' column.\n",
    "  - for each of these rows, we replace its 'earliest_date' value with that of whatever continuous feature column has the minimum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-naples",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some of the techniques for this cell were adapted from the two links below\n",
    "# https://stackoverflow.com/questions/13842088/set-value-for-particular-cell-in-pandas-dataframe-using-index\n",
    "# https://stackoverflow.com/questions/59879720/find-min-value-column-and-min-value-column-name-in-python-dataframe\n",
    "\n",
    "for row in failed_rows_earliest_date:\n",
    "    print(f'For row {row}, we will replace {df.loc[row][\"earliest_date\"]} with {df[continuous].loc[row].min()}')\n",
    "    df.at[row, \"earliest_date\"] = df.at[row, df[continuous].idxmin(axis='columns')[row]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-reducing",
   "metadata": {},
   "source": [
    "**Now we can perform the check again, on the entire dataframe to see if any rows in the dataframe have a date that is earlier than the value in the \"earliest_date\" column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_rows_earliest_date_2 = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i]['report_date'] < df.loc[i]['earliest_date'] or df.loc[i]['posSpec_date'] < df.loc[i]['earliest_date']\\\n",
    "    or df.loc[i]['onset'] < df.loc[i]['earliest_date']:\n",
    "        failed_rows_earliest_date_2.append(i)\n",
    "print(f'There are {len(failed_rows_earliest_date_2)} rows that have a date earlier than the \"earliest_column\" date.')\n",
    "print()\n",
    "print('Rows that have a date earlier than the \"earliest_date\" column:', failed_rows_earliest_date_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-burns",
   "metadata": {},
   "source": [
    "**As we can see, there are no rows whose continuous columns have a date earlier than its 'earliest_date' column.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-blogger",
   "metadata": {},
   "source": [
    "#### 3. Logical Integrity – 'status'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-workstation",
   "metadata": {},
   "source": [
    "- Here we 215 rows have a value of 'Probable Case' for the 'status' column, but also have a value for the 'posSpec_date' column – which indicates that they are a confirmed case.\n",
    "- The indexes of these rows are stored in the list 'failed_rows_PosSpec_date_status' from when the logical error tests were done.\n",
    "- We simply assign to the variable for the dataframe, the value of the dataframe that has dropped all rows with the indexes in the list 'failed_rows_PosSpec_date_status'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(failed_rows_PosSpec_date_status)} rows that have a \"probable_case\" for the status column, \\\n",
    "and a not null value for the posSpec_date column.')\n",
    "print()\n",
    "print(f'Rows that have a \"probable_case\" for the status column,\\\n",
    "and a not null value for the posSpec_date column: {failed_rows_PosSpec_date_status}.')\n",
    "\n",
    "df = df.drop(failed_rows_PosSpec_date_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-science",
   "metadata": {},
   "source": [
    "**Now we perform the check again, on the entire dataframe, to see if any rows have a value of 'Probable Case' for the 'status' column, but also have a value for the 'posSpec_date' column.**\n",
    "- We also need to reset the indexes for the dataframe, so that it can be iterated through without throwing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_rows_PosSpec_date_status_2 = []\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if not pd.isnull(df.loc[i]['posSpec_date']) and df.loc[i]['status'] == 'Probable Case':\n",
    "        failed_rows_PosSpec_date_status_2.append(i)\n",
    "        \n",
    "print(f'There are {len(failed_rows_PosSpec_date_status_2)} rows that have a \"probable_case\" for the status column, \\\n",
    "and a not null value for the posSpec_date column.')\n",
    "print()\n",
    "print(f'Rows that have a \"probable_case\" for the status column,\\\n",
    "and a not null value for the posSpec_date column: {failed_rows_PosSpec_date_status_2}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-mauritius",
   "metadata": {},
   "source": [
    "**As we can see, there are no rows that have a \"probable_case\" for the status column, and a not-null value for the posSpec_date column.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-riding",
   "metadata": {},
   "source": [
    "#### 4. Continuous features – 'posSpec_date'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-spanking",
   "metadata": {},
   "source": [
    "- Here we are dropping the 'posSpec_date' column from the dataframe.\n",
    "  - It is missing 72.74% of its values.\n",
    "  - It is not singly relied upon for the date in any row.\n",
    "- We simply assign to the dataframe, the value of the dataframe that has dropped the 'posSpec_date' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['posSpec_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-techno",
   "metadata": {},
   "source": [
    "**Now we check the dataframe to see if the 'posSpec_date' column is present.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-observer",
   "metadata": {},
   "source": [
    "**As we can see, the 'posSpec_date' column is not present in the dataframe anymore.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-adelaide",
   "metadata": {},
   "source": [
    "#### 5. Continuous features – 'onset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-carpet",
   "metadata": {},
   "source": [
    "- Here we are dropping the 'onset' column from the dataframe.\n",
    "  - It is missing 49.16% of its values.\n",
    "  - Similarly to the 'posSpec_date' column, it is not singly relied upon for the date in any row.\n",
    "- We simply assign to the dataframe, the value of the dataframe that has dropped the 'onset' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['onset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-edmonton",
   "metadata": {},
   "source": [
    "**Now we check to see if the dataframe contains an 'onset' column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-connecticut",
   "metadata": {},
   "source": [
    "**It can be seen that the dataframe has dropped the 'onset' column.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-asset",
   "metadata": {},
   "source": [
    "#### 6. Categorical features – 'icu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-venue",
   "metadata": {},
   "source": [
    "- Here we drop the 'icu' column from the dataframe.\n",
    "  - It is missing 89.35% of its values.\n",
    "- We assign to the dataframe, the value of the dataframe that has droppped the 'icu' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['icu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-glory",
   "metadata": {},
   "source": [
    "**Now we check to see if the dataframe contains an 'icu' column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-reward",
   "metadata": {},
   "source": [
    "**We can see that the dataframe no longer contains an 'icu' column.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-subscriber",
   "metadata": {},
   "source": [
    "#### 7. Categorical features – 'medcond'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-crazy",
   "metadata": {},
   "source": [
    "- We will now drop the 'medcond' column from the dataframe.\n",
    "  - It has 82.83% of its values missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['medcond'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-handling",
   "metadata": {},
   "source": [
    "**Check to see if the dataframe has successfully dropped the 'medcond' column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-classic",
   "metadata": {},
   "source": [
    "**We can see that the dataframe no longer has a 'medcond' column**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-piano",
   "metadata": {},
   "source": [
    "#### 8. Categorical features – 'sex'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-mouth",
   "metadata": {},
   "source": [
    "- Here we will iterate through the dataframe, and delete any rows to see if the value for the 'sex' column is missing or unknown.\n",
    "- We reset the indexes for the dataframe to make sure no errors are thrown when iterating through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i]['sex'] == 'Missing' or df.loc[i]['sex'] == 'Unknown':\n",
    "        df = df.drop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-tobago",
   "metadata": {},
   "source": [
    "**Now we make sure that there are no values of 'Missing' or 'Unknown' for the 'sex' column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-baking",
   "metadata": {},
   "source": [
    "**We can see that there are no values missing or unkown for the 'sex' column now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-dubai",
   "metadata": {},
   "source": [
    "#### 9. Categorical features – 'age_group'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-settle",
   "metadata": {},
   "source": [
    "- Finally we check the rows that have a value of 'Missing' or 'Unknown' for the 'age_group' column.\n",
    "- We reset the indexes of the dataframe before and after removing the rows to make sure no errors will be thrown for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i]['age_group'] == 'Missing' or df.loc[i]['age_group'] == 'Unknown':\n",
    "        df = df.drop(i)\n",
    "        \n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-planet",
   "metadata": {},
   "source": [
    "**Now we check that there are no rows that have a value of 'Missing' or 'Unknown' for the 'age_group' column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-constitutional",
   "metadata": {},
   "source": [
    "**As we can see, there are no values of 'Missing' or 'Unknown' for the 'age_group' column.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-hurricane",
   "metadata": {},
   "source": [
    "### Summary of data quality plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-papua",
   "metadata": {},
   "source": [
    "**Variable Names**|**Data Quality Issue**|**Handling Strategy**\n",
    ":-----:|:-----:|:-----:\n",
    "earliest\\_date|logical errors|change value for  the 'earliest\\_date' column to the minimum value for the column\n",
    "report\\_date|missing 24.08% of its values.|do nothing\n",
    "posSpec\\_date|missing 72.74% of its values|drop column\n",
    "onset|missing 49.16 % of its values; imperfect method of data collection|drop column\n",
    "status|logical errors|remove failed rows\n",
    "sex|missing 0.82% of its values|remove rows with missing values\n",
    "age\\_group|missing 0.1% of its values|remove rows with missing values\n",
    "race\\_ethnicity|missing 42.54% of its values|do nothing\n",
    "hosp|missing 41.25% of its values|do nothing\n",
    "icu|missing 89.35% of its values|drop column\n",
    "death|none|do nothing\n",
    "medcond|missing 82.83% of its values|do nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-variable",
   "metadata": {},
   "source": [
    "- table saved as picture in Data_Quality_Plan.pdf\n",
    "- table made with excel and pasted into generator here:\n",
    "https://jakebathman.github.io/Markdown-Table-Generator/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-relations",
   "metadata": {},
   "source": [
    "**The dataframe with descriptive statistics of its continuous features now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = df.select_dtypes('datetime64[ns]').columns\n",
    "df[continuous].describe(datetime_is_numeric=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-cloud",
   "metadata": {},
   "source": [
    "**The dataframe with descriptive statistics of its categorical features now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = df.select_dtypes('category').columns\n",
    "df[categorical].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-singer",
   "metadata": {},
   "source": [
    "## Save the new dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('covid19-cdc-19200690_cleaned_1_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-ownership",
   "metadata": {},
   "source": [
    "# (3). Exploring relationships between feature pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-gather",
   "metadata": {},
   "source": [
    "## - Choose a subset of features you find promising and plot pairwise feature interactions. Explain your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-meeting",
   "metadata": {},
   "source": [
    "I will look at the following continuous feature for pairwise interactions:\n",
    "- earliest_date\n",
    "<br><br>\n",
    "\n",
    "I will look at the following categorical features for pairwise interactions:\n",
    "- status\n",
    "- sex\n",
    "- age_group\n",
    "- race_ethnicity\n",
    "- hosp\n",
    "- death"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-blast",
   "metadata": {},
   "source": [
    "I have chosen each of these features because each one can help with identifying what is likely to contribute to a death in relation to COVID-19 cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-management",
   "metadata": {},
   "source": [
    "## Categorical vs Categorical plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-pricing",
   "metadata": {},
   "source": [
    "### - Comparing sex with status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from:\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n",
    "\n",
    "plt.style.use('classic')\n",
    "\n",
    "titles_sex_status = ['Total numbers', 'Laboratory-confirmed case', 'Probable case']\n",
    "male = df['sex']=='Male'\n",
    "female = df['sex']=='Female'\n",
    "confirmed = df['status']=='Laboratory-confirmed case'\n",
    "probable = df['status']=='Probable Case'\n",
    "\n",
    "male_confirmed = df[male & confirmed]\n",
    "male_probable = df[male & probable]\n",
    "\n",
    "female_confirmed = df[female & confirmed]\n",
    "female_probable = df[female & probable]\n",
    "\n",
    "total_percentage_male = 100 * (len(df[male])/df.shape[0])\n",
    "total_percentage_female = 100 * (len(df[female])/df.shape[0])\n",
    "\n",
    "confirmed_percentage_male = 100 * (len(male_confirmed)/len(df[confirmed]))\n",
    "confirmed_percentage_female = 100 * (len(female_confirmed)/len(df[confirmed]))\n",
    "\n",
    "probable_percentage_male = 100 * (len(male_probable)/len(df[probable]))\n",
    "probable_percentage_female = 100 * (len(female_probable)/len(df[probable]))\n",
    "\n",
    "\n",
    "male_list = [total_percentage_male, confirmed_percentage_male, probable_percentage_male]\n",
    "female_list = [total_percentage_female, confirmed_percentage_female, probable_percentage_female]\n",
    "\n",
    "x = np.arange(len(titles_sex_status))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, male_list, width, label='Male')\n",
    "rects2 = ax.bar(x + width/2, female_list, width, label='Female')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentages')\n",
    "ax.set_title('Scores by group and gender')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(titles_sex_status)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-symbol",
   "metadata": {},
   "source": [
    "### - Comparing age group with status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n",
    "\n",
    "titles_age_status = [title for title in df['age_group'].unique()]\n",
    "titles_age_status.sort()\n",
    "\n",
    "zero = df['age_group']=='0 - 9 Years'\n",
    "ten = df['age_group']=='10 - 19 Years'\n",
    "twenty = df['age_group']=='20 - 29 Years'\n",
    "thirty = df['age_group']=='30 - 39 Years'\n",
    "forty = df['age_group']=='40 - 49 Years'\n",
    "fifty = df['age_group']=='50 - 59 Years'\n",
    "sixty = df['age_group']=='60 - 69 Years'\n",
    "seventy = df['age_group']=='70 - 79 Years'\n",
    "eighty = df['age_group']=='80+ Years'\n",
    "\n",
    "confirmed = df['status']=='Laboratory-confirmed case'\n",
    "probable = df['status']=='Probable Case'\n",
    "\n",
    "age_categories = [zero, ten, twenty, thirty, forty, fifty, sixty, seventy, eighty]\n",
    "total_percentage_list = []\n",
    "confirmed_percentage_list = []\n",
    "probable_percentage_list = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(age_categories)):\n",
    "    total_percentage_list.append(100*(len(df[age_categories[i]])/df.shape[0]))\n",
    "    confirmed_percentage_list.append(100*(len(df[age_categories[i] & confirmed])/len(df[confirmed])))\n",
    "    probable_percentage_list.append(100*(len(df[age_categories[i] & probable])/len(df[probable])))\n",
    "\n",
    "\n",
    "x = np.arange(len(titles_age_status))  # the label locations\n",
    "width = 0.3  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, total_percentage_list, width, label='Total')\n",
    "rects2 = ax.bar(x, confirmed_percentage_list, width, label='Confirmed')\n",
    "rects3 = ax.bar(x + width, probable_percentage_list, width, label='Probable')\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentages')\n",
    "ax.set_title('Percentages by age group')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(titles_age_status, rotation=90)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-stranger",
   "metadata": {},
   "source": [
    "### - Comparing race & ethnicity with status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n",
    "\n",
    "titles_race_ethnicity = [title for title in df['race_ethnicity'].unique()]\n",
    "titles_race_ethnicity.sort()\n",
    "\n",
    "confirmed = df['status']=='Laboratory-confirmed case'\n",
    "probable = df['status']=='Probable Case'\n",
    "\n",
    "total_percentage_list = []\n",
    "confirmed_percentage_list = []\n",
    "probable_percentage_list = []\n",
    "\n",
    "for i in range(len(titles_race_ethnicity)):\n",
    "    total_percentage_list.append(100*(len(df[df['race_ethnicity'] == titles_race_ethnicity[i]])/df.shape[0]))\n",
    "    confirmed_percentage_list.append(100*(len(df[(df['race_ethnicity'] == titles_race_ethnicity[i]) & confirmed])/len(df[confirmed])))\n",
    "    probable_percentage_list.append(100*(len(df[(df['race_ethnicity'] == titles_race_ethnicity[i]) & probable])/len(df[probable])))\n",
    "\n",
    "x = np.arange(len(titles_age_status))  # the label locations\n",
    "width = 0.3  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, total_percentage_list, width, label='Total')\n",
    "rects2 = ax.bar(x, confirmed_percentage_list, width, label='Confirmed')\n",
    "rects3 = ax.bar(x + width, probable_percentage_list, width, label='Probable')\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentages')\n",
    "ax.set_title('Percentages by race/ethnicity')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(titles_race_ethnicity, rotation=90)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-ottawa",
   "metadata": {},
   "source": [
    "### - Comparing sex with death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_sex_death = ['Total', 'dead', 'not dead']\n",
    "\n",
    "total_percent_male = 100*len(df[df['sex']=='Male'])/df.shape[0]\n",
    "total_percent_female = 100*len(df[df['sex']=='Female'])/df.shape[0]\n",
    "\n",
    "death_yes_percent_male = 100*len(df[(df['sex']=='Male')&(df['death']=='Yes')])/len(df[df['death']=='Yes'])\n",
    "death_yes_percent_female = 100*len(df[(df['sex']=='Female')&(df['death']=='Yes')])/len(df[df['death']=='Yes'])\n",
    "\n",
    "death_no_percent_male = 100*len(df[(df['sex']=='Male')&(df['death']=='No')])/len(df[df['death']=='No'])\n",
    "death_no_percent_female = 100*len(df[(df['sex']=='Female')&(df['death']=='No')])/len(df[df['death']=='No'])\n",
    "\n",
    "male_list = [total_percent_male, death_yes_percent_male, death_no_percent_male]\n",
    "female_list = [total_percent_female, death_yes_percent_female, death_no_percent_female]\n",
    "\n",
    "x = np.arange(len(titles_sex_status))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, male_list, width, label='Male')\n",
    "rects2 = ax.bar(x + width/2, female_list, width, label='Female')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentages')\n",
    "ax.set_title('Comparison of sex and death')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(titles_sex_death)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-completion",
   "metadata": {},
   "source": [
    "### - Comparing age group with deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n",
    "\n",
    "titles_age_death = [title for title in df['age_group'].unique()]\n",
    "titles_age_death.sort()\n",
    "\n",
    "zero = df['age_group']=='0 - 9 Years'\n",
    "ten = df['age_group']=='10 - 19 Years'\n",
    "twenty = df['age_group']=='20 - 29 Years'\n",
    "thirty = df['age_group']=='30 - 39 Years'\n",
    "forty = df['age_group']=='40 - 49 Years'\n",
    "fifty = df['age_group']=='50 - 59 Years'\n",
    "sixty = df['age_group']=='60 - 69 Years'\n",
    "seventy = df['age_group']=='70 - 79 Years'\n",
    "eighty = df['age_group']=='80+ Years'\n",
    "\n",
    "death_yes = df['death']=='Yes'\n",
    "death_no = df['death']=='No'\n",
    "\n",
    "age_categories = [zero, ten, twenty, thirty, forty, fifty, sixty, seventy, eighty]\n",
    "total_percentage_list = []\n",
    "death_percentage_list = []\n",
    "not_death_percentage_list = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(age_categories)):\n",
    "    total_percentage_list.append(100*(len(df[age_categories[i]])/df.shape[0]))\n",
    "    death_percentage_list.append(100*(len(df[age_categories[i] & death_yes])/len(df[death_yes])))\n",
    "    not_death_percentage_list.append(100*(len(df[age_categories[i] & death_no])/len(df[death_no])))\n",
    "\n",
    "\n",
    "x = np.arange(len(titles_age_death))  # the label locations\n",
    "width = 0.3  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, total_percentage_list, width, label='Total')\n",
    "rects2 = ax.bar(x, death_percentage_list, width, label='Dead')\n",
    "rects3 = ax.bar(x + width, not_death_percentage_list, width, label='Not dead')\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentages')\n",
    "ax.set_title('Percentages by age group')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(titles_age_status, rotation=90)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-berlin",
   "metadata": {},
   "source": [
    "### - Comparing ethnicity with deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n",
    "\n",
    "titles_race_ethnicity = [title for title in df['race_ethnicity'].unique()]\n",
    "titles_race_ethnicity.sort()\n",
    "\n",
    "death_yes = df['death']=='Yes'\n",
    "death_no = df['death']=='No'\n",
    "\n",
    "total_percentage_list = []\n",
    "death_percentage_list = []\n",
    "not_death_percentage_list = []\n",
    "\n",
    "for i in range(len(titles_race_ethnicity)):\n",
    "    total_percentage_list.append(100*(len(df[df['race_ethnicity'] == titles_race_ethnicity[i]])/df.shape[0]))\n",
    "    death_percentage_list.append(100*(len(df[(df['race_ethnicity'] == titles_race_ethnicity[i]) & death_yes])/len(df[death_yes])))\n",
    "    not_death_percentage_list.append(100*(len(df[(df['race_ethnicity'] == titles_race_ethnicity[i]) & death_no])/len(df[death_no])))\n",
    "\n",
    "x = np.arange(len(titles_race_ethnicity))  # the label locations\n",
    "width = 0.3  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, total_percentage_list, width, label='Total')\n",
    "rects2 = ax.bar(x, death_percentage_list, width, label='Dead')\n",
    "rects3 = ax.bar(x + width, not_death_percentage_list, width, label='Not dead')\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentages')\n",
    "ax.set_title('Percentages by race/ethnicity')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(titles_race_ethnicity, rotation=90)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-farmer",
   "metadata": {},
   "source": [
    "### - Comparing ethnicity with hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n",
    "\n",
    "titles_race_ethnicity = [title for title in df['race_ethnicity'].unique()]\n",
    "titles_race_ethnicity.sort()\n",
    "\n",
    "hosp_yes = df['hosp']=='Yes'\n",
    "hosp_no = df['hosp']=='No'\n",
    "\n",
    "total_percentage_list = []\n",
    "hosp_yes_percentage_list = []\n",
    "hosp_no_percentage_list = []\n",
    "\n",
    "for i in range(len(titles_race_ethnicity)):\n",
    "    total_percentage_list.append(100*(len(df[df['race_ethnicity'] == titles_race_ethnicity[i]])/df.shape[0]))\n",
    "    hosp_yes_percentage_list.append(100*(len(df[(df['race_ethnicity'] == titles_race_ethnicity[i]) & hosp_yes])/len(df[hosp_yes])))\n",
    "    hosp_no_percentage_list.append(100*(len(df[(df['race_ethnicity'] == titles_race_ethnicity[i]) & hosp_yes])/len(df[hosp_yes])))\n",
    "\n",
    "x = np.arange(len(titles_race_ethnicity))  # the label locations\n",
    "width = 0.3  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, total_percentage_list, width, label='Total')\n",
    "rects2 = ax.bar(x, hosp_yes_percentage_list, width, label='In hospital')\n",
    "rects3 = ax.bar(x + width, hosp_no_percentage_list, width, label='Not in hospital')\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentages')\n",
    "ax.set_title('Percentages by race/ethnicity')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(titles_race_ethnicity, rotation=90)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-monster",
   "metadata": {},
   "source": [
    "## - comparing number of hospitalisations with deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_yes = df['hosp']=='Yes'\n",
    "hosp_no = df['hosp']=='No'\n",
    "\n",
    "dead_yes = df['death']=='Yes'\n",
    "dead_no = df['death']=='No'\n",
    "\n",
    "titles_hosp = [\"in hospital\", 'not in hosptial']\n",
    "\n",
    "hosp_dead = 100*(len(df[hosp_yes & dead_yes])/len(df[hosp_yes]))\n",
    "hosp_not_dead = 100*(len(df[hosp_yes & dead_no])/len(df[hosp_yes]))\n",
    "\n",
    "no_hosp_dead = 100*(len(df[hosp_no & dead_yes])/len(df[hosp_no]))\n",
    "no_hosp_no_dead = 100*(len(df[hosp_no & dead_no])/len(df[hosp_no]))\n",
    "\n",
    "hosp_list = [hosp_dead, hosp_not_dead]\n",
    "no_hosp_list = [no_hosp_dead, no_hosp_no_dead]\n",
    "\n",
    "x = np.arange(len(titles_hosp))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, hosp_list, width, label='In hospital')\n",
    "rects2 = ax.bar(x + width/2, no_hosp_list, width, label='Not in hospital')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentages')\n",
    "ax.set_title('Deaths compared with hospitalisation')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Death', 'No death'])\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "no_hosp_no_dead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-norway",
   "metadata": {},
   "source": [
    "# Continuous vs Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-detective",
   "metadata": {},
   "source": [
    "## - Plotting number of deaths per date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateNums = {}\n",
    "uniqueDates = df['earliest_date'].unique()\n",
    "uniqueDates.sort()\n",
    "\n",
    "deaths = df[df['death']=='Yes']\n",
    "\n",
    "for date in uniqueDates:\n",
    "    dateNums[date] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in uniqueDates:\n",
    "    for i in range(len(deaths)):\n",
    "        if deaths.iloc[i]['earliest_date'] == date:\n",
    "            dateNums[date]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n",
    "plt.plot_date(uniqueDates, dateNums.values(), linestyle='solid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-poetry",
   "metadata": {},
   "source": [
    "## - Plotting number of deaths per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths = df[df['death']=='Yes']['earliest_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Number of deaths per month')\n",
    "plt.style.use('classic')\n",
    "deaths.hist(figsize=(10, 10), xrot = 90, edgecolor='white', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-polymer",
   "metadata": {},
   "source": [
    "## - Plotting number of hospitalisations per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosps = df[df['hosp']=='Yes']['earliest_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Number of hospitalisations per month')\n",
    "plt.style.use('classic')\n",
    "deaths.hist(figsize=(10, 10), xrot = 90, edgecolor='white', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-rabbit",
   "metadata": {},
   "source": [
    "## Findings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-gregory",
   "metadata": {},
   "source": [
    "- categorical vs categorical\n",
    "  - I picked groups of categorical features that I thought I could compare with specific outcomes: such as how race_ethnicity correlated with deaths and confirmed positive cases.\n",
    "  - There is a strong correlation between going to hospital and death.\n",
    "  - There is also a very clear correlation between the deaths and increasing age – the percentage of deaths is far higher for the '60 - 69 Years', '70 - 79 Years' and '80+ Years' categories.\n",
    "- continuous vs categorical\n",
    "  - There were no findings here other than a pattern of the number of cases mirroring that of deaths: when the number of cases rose April 2020, so did the number of deaths. This was the same as the year went progressed, and the cases and deaths increased again in December.\n",
    "  - There was also more evidence of correlation with hospitalisation and deaths: the more hospitalisations, the more deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-franchise",
   "metadata": {},
   "source": [
    "# (4). Transform, extend or combine the existing features to create a few new features (at least 3) with the aim to better capture the problem domain and the target outcome.\n",
    "### Justify the steps and choices you are making. Add these features to your clean dataset and save it as a CSV file with a self explanatory name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-peninsula",
   "metadata": {},
   "source": [
    "## Time difference\n",
    "- This is interesting to track, as it is important to keep figures up to date, so that more data can be gathered and collated to continue to help identifying trends. Perhaps we may see that if the time difference between when the case may be first diagnosed or tested increases, the number of deaths may increase. This could also be indicative of hospital stress – if staff are too busy with patients, administrative work may be delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dif = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    time_dif.append((df.loc[i]['report_date'])-(df.loc[i]['earliest_date']))\n",
    "    \n",
    "df['time_dif'] = time_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-inflation",
   "metadata": {},
   "source": [
    "## Elderly\n",
    "- Because the percentage of deaths went up from 60 years onwards, I thought it would be a good feature to have on the dataset to indicate that patients who are elderly will receive more attention. As we can see below, over 80% of the total deaths occur in elderly people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "elderly = []\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i]['age_group'] == '60 - 69 Years' or df.loc[i]['age_group'] == '70 - 79 Years' \\\n",
    "    or df.loc[i]['age_group'] == '80+ Years':\n",
    "        elderly.append('Yes')\n",
    "    else:\n",
    "        elderly.append('No')\n",
    "        \n",
    "df['elderly'] = elderly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "100* len(df[(df['death']=='Yes')&(df['elderly']=='Yes')])/len(df[df['death']=='Yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-lingerie",
   "metadata": {},
   "source": [
    "## Quarter\n",
    "- In the Data Quality Report, there were statistics on the continuous features, in terms of percentiles. But What I thought would be useful is to see how the cases are distributed according to time. So I made a quarter feature. If the case occurs in the first quartile of the span of time that cases are recorded, then it is designated as 'first', if the case occurs in the second quartile of the span of time that cases are recorded, then it is designated as 'second', and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[continuous].describe(datetime_is_numeric=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterList = []\n",
    "\n",
    "quarter = (max(df['earliest_date']) - min(df['earliest_date']))/4\n",
    "first = min(df['earliest_date']) + quarter\n",
    "second = min(df['earliest_date']) + (2*quarter)\n",
    "third = max(df['earliest_date']) - quarter\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i]['earliest_date'] >= min(df['earliest_date']) and \\\n",
    "    df.loc[i]['earliest_date'] <= first:\n",
    "        quarterList.append('first')\n",
    "    elif df.loc[i]['earliest_date'] >= first and \\\n",
    "    df.loc[i]['earliest_date'] <= second:\n",
    "        quarterList.append('second')\n",
    "    elif df.loc[i]['earliest_date'] >= second and \\\n",
    "    df.loc[i]['earliest_date'] <= third:\n",
    "        quarterList.append('third')\n",
    "    else:\n",
    "        quarterList.append('fourth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quarter'] = quarterList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['quarter']=='first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('covid19-cdc-19200690_cleaned_1_3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
